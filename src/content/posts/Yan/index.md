---
title: Yan：Foundational Interactive Video Generation阅读笔记
published: 2025-12-11 17:28:55
slug: yan
tags: ['计算机视觉', '视频生成', '交互式生成']
category: 计算机视觉
draft: false
---

# Yan: Foundational Interactive Video Generation

## 摘要

- Yan，一个覆盖模拟，生成，到编辑全流程的交互式视频生成基础框架。
- 三个核心模块
- AAA-level Simulation
- Multi-Modal Generation
- Multi-Granulartiy Editing

## 引言

- 交互式生成视频(IGV)
- 现有问题：
	- 高视觉保真度
	- 长期的时序一致性
	- 丰富而稳定的交互能力
- 现代电子游戏
	- 1080P / 60FPS 的实时渲染能力
	- 复杂物理机制模拟
	- 对多样输入的细粒度反馈
	- 可动态改变的内容结构

## 相关工作

- 交互式生成视频
- 相关研究主要分为两大类
	- 以游戏为中心的交互式视频生成
	- 真实世界 World Models
- 神经网络驱动的游戏模拟
	- 分辨率和帧率低
- 视频编辑
- Yan在编辑上的独特性

## 概述

![](img/Pasted%20image%2020251211172827.png)

1. 自动化数据采集
	- 在3D游戏环境中采集交互式视频数据
2. 数据标注
	- Qwen2.5-VL，DepthFM-ID 自动标注
	- 生成结构化文本，估计深度图
3. 三大模块

## 数据收集

![](img/Pasted%20image%2020251211204955.png)

- 数据采集流水线

- 探索Agent
	- 输入 环节图像ot
	- 输出 动作信号 at
	- agent策略：随机模型，PPO

- 图像-动作对序列采集
	- 动作信号-时间戳
	- 1080p截图

- 数据过滤
	- 视觉过滤 删除画面错误/遮挡严重的帧
		- 计算每帧颜色方差
	- 异常过滤 删除包含卡顿/重复帧的低质量片段
	- 规则过滤 删除与游戏机制不一致的数据

- 数据平衡器

- 高质量交互视频数据
	- 高分辨率
	- 高帧率
	- 高精度图像-动作对齐
	- 丰富的动作空间
	- 多样场景
	- 对比
	![](img/Pasted%20image%2020251211205726.png)

- 数据总结


## 方法

### 模拟

![](img/Pasted%20image%2020251211211242.png)

- 模型架构

	- VAE
	- 更强的空间压缩  下采样因子(32x->8x)
	- 时间压缩 拼接相邻帧
	- 潜空间通道数 C=16
	- Decoder轻量化 降低推理开销
	
	- Diffusion Model设计 逐帧自回归生成
	- 空间注意力，时间注意力，动作条件注意力
	- 动作信号注入 使用MLP将动作编码为768维 token
	- 因果时间注意力 只能访问前面的帧

- 训练

	 - VAE
	- 输入连续两帧拼接后的图像
	- 输出大小为hxwx16的潜空间表示
	- 损失函数 MSE,LPIPS

	- 扩散模型训练
	- DDPM框架
	- 引入 Diffusion Forcing
	- 每帧独立增加噪声
	- 第一帧clean
	- 随时间逐渐提升噪声等级

- 推理

	- 4步DDIM
	- 滑动窗口去噪 KV cache
	- UNet剪枝，矩阵乘法量化

- 评估

- 视觉质量

![](img/Pasted%20image%2020251211211649.png)

-  运动一致性

![](img/Pasted%20image%2020251211211829.png)

- 物理机制模拟

![](img/Pasted%20image%2020251211211854.png)

- 长视频生成能力

![](img/Pasted%20image%2020251211211910.png)


### 生成

![](img/Pasted%20image%2020251211212005.png)
#### 分层字幕 全局+局部

- 全局字幕——定义世界
- 局部字母——对应短时事件
- 使用Qwen2.5-VL

![](img/Pasted%20image%2020251211220051.png)

#### 多模态条件——互动世界生成

- 文本/图像生成能力迁移
	- 预训练Wan模型
	- 文本-视频和图像-视频分支所有线性层用LoRA微调
	- 使用分层字幕作为输入条件
	- 10%换成全局字幕
	
- 动作条件注入
	- 独立动作编码器MLP
	- 动作token注入每个Dit Block独立的cross-attention分支
	- 使用与Sim一致的因果注意力掩码
	- 10%替换成空动作
	
- 训练目标
	- Rectified Flows
![](img/Pasted%20image%2020251211215726.png)
- 其中
	- 文本 embedding：UMT5-XXL（512 tokens）
	- 图像 embedding：ViT-H-14（257 tokens）
	- 动作序列：逐帧 action signals

- 推理公式
	- 通过classifier-free guidance 合成
![](img/Pasted%20image%2020251211215830.png)

#### 自回归后训练

- 为了支持实时互动+无限生成
- 因果式，少步数的自回归视频生成器

1. 生成ODE轨迹
2. 加入因果注意力

#### 自引导蒸馏

- 更少步数推理，更稳定长序列生成，更适于实时互动

1. DMD 匹配生成分布与真实分布

![](img/Pasted%20image%2020251212191033.png)

2. 一步生成器扩展到少步生成器
3. 缩小暴走误差 self-forcing

- 最终性能
- 单张H20 12-17FPS
- 四卡并行 30FPS

#### 评估

1. 文本驱动互动

![](img/Pasted%20image%2020251212191503.png)

物理响应

2. 文本引导世界扩展

![](img/Pasted%20image%2020251212191539.png)

3. 图像驱动互动

![](img/Pasted%20image%2020251212191633.png)

4.跨领域融合

![](img/Pasted%20image%2020251212191711.png)




### 编辑（Yan-Edit）

#### 模型设计与架构

- **交互式视频编辑核心要求**  
  编辑后的内容 **仍需保持正确、可交互的物理与机制行为**，而不仅是外观一致。  
  - 例：把赛车从“蓝色”改成“绿色”，仍要保持原本的转向、碰撞、加速等交互机制一致。

- **核心思想：混合式架构（Hybrid Architecture）**  
  将以下两部分进行 **显式分离（Disentanglement）**：
  - **交互机制模拟（Interactive Mechanics Simulation）**：负责“结构 → 物理/机制/交互”
  - **视觉渲染（Visual Rendering）**：负责“结构 → 外观/风格/材质”

![](img/Pasted%20image%2020251212192518.png)

- **为什么用深度（Depth）做桥梁？**
  - 深度图保留三维结构与几何关系
  - 去除了颜色/纹理等外观信息
  - 使“交互机制学习”更偏向结构依赖、可泛化，避免被风格干扰

---

#### 整体流程（Overall Pipeline）

1. **提取深度图**
   - 从交互视频中提取每帧深度图
   - 目标：保留三维结构、去除外观渲染信息

2. **基于深度图学习与结构相关的交互机制**
   - 学习结构依赖的物理/交互规律
   - 对颜色、材质变化保持鲁棒（style-invariant mechanics）

3. **由视觉渲染器将深度图渲染为最终视频帧**
   - 将深度序列渲染成最终 RGB 视频
   - 支持开放域风格、材质与颜色变化

---

#### 两类文本提示（Text Prompts）

- **结构提示（Structure Prompt）**
  - 控制：场景结构、物体类型、机关/可交互元素、交互逻辑
  - 作用对象：交互机制模拟器（影响“世界结构 + 交互规律”）
  - 示例：添加“圆柱风扇/弹跳床”、把“旋转平台”替换成“木门”

- **风格提示（Style Prompt）**
  - 控制：颜色、材质、纹理、整体视觉风格（可跨域）
  - 作用对象：视觉渲染器（影响“外观渲染”）
  - 示例：水彩竹林、紫色玻璃+青色霓虹、雪景粉彩等

---

#### 框架组成（两模块）

##### 1) 交互机制模拟器（Interactive Mechanics Simulator）

- **定位**
  - 只负责“交互机制/物理”与“结构演化”
  - 输出：下一帧 **深度图**（而不是 RGB）

- **基座模型**
  - 以 **Yan-Sim** 为基础（但输入/输出改为深度域）
  - 依靠 Yan-Sim 的强交互模拟能力来学习 structure-dependent mechanics

- **条件输入**
  - 初始深度（Init Depth）
  - 用户逐帧动作信号（Action Signal，例如键盘 WASD）
  - 结构提示（Structure Prompt）

- **结构提示如何注入？（Text Cross-Attention 插入位置）**
  - 在 Yan-Sim 的注意力块中：
    - 先做 **动作 cross-attention**
    - 再插入 **文本 cross-attention（结构提示）**
  - 目的：让结构提示直接影响“机制生成”，实现结构级编辑

---

##### 2) 视觉渲染器（Visual Renderer）

- **定位**
  - 只负责“外观渲染/风格材质”
  - 输入：深度序列 + 风格提示
  - 输出：最终 RGB 帧

- **基座模型**
  - 以 **Yan-Gen** 为基础（开放域生成能力强）

- **深度条件注入方式：ControlNet**
  - 将深度帧通过 **ControlNet** 注入 Yan-Gen
  - ControlNet 由若干个从 Yan-Gen 复制的 DiT blocks 组成
  - 每层 ControlNet 输出 **加回** 到对应的原 DiT blocks（残差注入方式类似 VACE）

- **风格控制**
  - 使用 **Style Prompt** 控制颜色/纹理/材质/风格
  - 与交互机制解耦：风格改变不应破坏物理与交互规律

---

#### 训练与推理（Training & Inference）

##### 1) 交互机制模拟器训练（Depth-Mechanics Training）

- **(a) 深度图生成**
  - 使用 **DepthFM** 从视频抽取深度
  - 先训练专用的 **Depth VAE**（深度域编码/解码）

- **(b) 结构理解与标注（Structure Prompt 构建）**
  - 将交互视频切分为 **81 帧** temporal chunks
  - 使用 **Qwen2.5-VL** 做三维结构分解，形成结构提示
  - 结构分解的三类元素：
    1. 动态物体（Dynamic Objects）
    2. 墙体结构（Wall Structures）
    3. 环境平台（Environmental Platforms）

- **(c) 训练策略（两阶段）**
  - **阶段一：同时训练动作 + 文本 cross-attention**
    - 目的：让模型快速学会“动作驱动机制”与“结构编辑信号”的对齐
  - **阶段二：冻结文本注意力，仅微调动作注意力**
    - 目的：进一步提升动作响应一致性与物理稳定性，避免文本干扰交互规律

---

##### 2) 视觉渲染器训练（Depth→RGB Rendering Training）

- **训练目标**
  - 实现：
    - 多风格（in-domain + out-of-domain）
    - 时序一致（temporal consistency）
    - 深度驱动的稳定渲染

- **标准流程（理论上）**
  1. 先按 Yan-Gen 的方式训练“非因果”深度条件生成模型（对应 5.2.2）
  2. 再按 Yan-Gen 后训练方法蒸馏成“少步因果模型”（对应 5.2.3 / 5.2.4）

- **实践发现（关键工程结论）**
  - 直接将：
    - **Yan-Gen（未后训练阶段的版本）**
    - 与 **VACE 的开源 ControlNet 权重**
  - 组合后已能实现高质量、时序一致的深度渲染  
  ⇒ 因此只需要对“组合模型”继续做 **few-step causal 蒸馏**，即可满足实时交互需求。

- **蒸馏数据构建**
  - 使用已训练好的 **交互机制模拟器** 生成深度视频（每段 **81 帧**）
  - 随机输入：
    - 初始深度（random init depth）
    - 动作信号（random action）
    - 结构提示（random structure prompt）
  - 风格提示（Style Prompts）来源：
    - **in-domain**：从 5.2.1 分层字幕里抽取“风格描述部分”
    - **out-of-domain**：由 GPT-4 生成开放域风格描述（增强跨域渲染能力）

---

#### 推理阶段（Inference）：逐帧自回归（Frame-wise Auto-regressive）

1) **交互机制模拟器**
- 输入：
  - 用户动作信号（Action）
  - 结构提示（Structure Prompt）
  - 上一帧深度（Depth）
- 输出：
  - 下一帧深度（Next Depth）

2) **视觉渲染器**
- 输入：
  - 当前深度（Depth）
  - 风格提示（Style Prompt）
- 输出：
  - 当前帧 RGB（Final Frame）

- **KV Cache（时序一致性机制）**
  - 两个模块都可将历史信息写入 KV Cache
  - 目的：
    - 保证时间一致性
    - 支持长序列稳定生成（infinite-length）
    - 提升实时推理效率（减少重复计算）

- **关键能力（On-the-fly Editing）**
  - 在交互过程中任意时刻可修改：
    - 结构提示（立即改变场景结构/机关/可交互物体）
    - 风格提示（立即改变颜色/材质/整体风格）
  - 修改即时生效，不中断交互生成流程

---

#### 定性结果与能力验证（Qualitative Evidence）

- **结构编辑（Structure Editing）**
  - 可将场景元素即时替换（如 Rotate Plat ↔ Wooden Door）
  - 可生成新结构以克服障碍（如 Cylinder Fan / Trampoline）
  - 体现：内容创建 + 实时替换 + 保持交互机制一致

![](img/Pasted%20image%2020251212194840.png)

- **风格编辑（Style Editing）**
  - 可在生成过程中多次切换风格提示（如在第 81/162 帧切换）
  - 风格过渡连续，时序一致性较好
  - 同时保持交互机制不被风格变化破坏



## 局限性与结论
---

### 一、局限性（Limitation）

尽管 Yan 在交互式视频的 **模拟、生成与编辑** 方面达到了当前先进水平，但仍存在以下主要局限：

---

![](img/Pasted%20image%2020251212194918.png)

####  长时序视觉一致性仍具挑战

- 在 **长时间生成**、尤其是 **复杂且快速变化的交互场景** 中：
  - 空间一致性
  - 时间一致性  
  仍可能逐渐退化
- 需要更强的长期稳定建模能力来缓解语义与视觉漂移问题

---

####  对高性能硬件的依赖较强

- 当前的高保真、实时性能依赖于 **高端 GPU 推理**
- 这限制了：
  - 普通用户的可访问性
  - 在低算力或边缘设备上的部署

- 潜在改进方向：
  - 更高效的模型架构
  - 更激进的压缩与蒸馏策略
  - 面向低资源设备的专项优化

---

####  交互复杂度受限于底层环境

- 虽然 Yan 对 in-domain 与 out-of-domain 场景均表现出良好泛化性
- 但：
  - 动作空间
  - 交互复杂度  
  仍受限于底层游戏环境设计
- 可能限制其向某些真实世界应用的扩展

---

####  编辑方式主要依赖文本提示

- Yan-Edit 当前以 **文本描述** 作为主要编辑控制方式
- 虽支持结构级与风格级编辑，但：
  - 交互形式仍不够直观
- 未来可探索：
  - 更自然的人机交互方式
  - 更丰富的编辑控制接口

---

### 二、结论（Conclusion）

Yan 是一个 **统一的交互式视频生成基础框架**，系统性整合了以下三项关键能力：

1. **高保真、实时的交互式模拟**
2. **基于提示可控的多模态生成**
3. **多粒度、文本驱动的交互式编辑**

所有能力均建立在一个 **大规模、精确标注的交互式视频数据集** 之上。

---

#### 核心贡献总结

- 通过模块化但高度协同的设计，Yan 成功打通：
  - 模拟（Simulation）
  - 创作（Generation）
  - 交互中即时定制（On-the-fly Editing）

- 推动交互式生成视频：
  - 从碎片化研究原型
  - 迈向统一、可扩展的 AI 内容引擎范式

---

#### 未来工作方向

作者提出的未来研究重点包括：

1. 扩展数据规模与模型容量，同时提升整体效率
2. 将提示可控的交互生成拓展到更复杂的真实世界场景
3. 构建同时考虑 **人因（Human Factors）与安全性** 的评估体系

---

#### 总结性观点

Yan 被视为迈向下一代 **开放式、可创造、AI 驱动交互世界** 的重要一步，为交互式媒体与世界模型的发展提供了可行路径。
